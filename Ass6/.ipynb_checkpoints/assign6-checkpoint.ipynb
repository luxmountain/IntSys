{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06b6e3b-6abf-4ed5-99e3-71be8773bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved itemReview_pxquy.csv\n"
     ]
    }
   ],
   "source": [
    "# B22DCAT240-Ph·∫°m Xu√¢n Qu√Ω\n",
    "import csv, random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "NUM_USERS = 1000\n",
    "NUM_ITEMS = 500\n",
    "REVIEWS_PER_USER = 50\n",
    "users = [f\"user_{i:04d}\" for i in range(1, NUM_USERS+1)]\n",
    "items = [f\"item_{i:04d}\" for i in range(1, NUM_ITEMS+1)]\n",
    "\n",
    "templates_pos = [\"S·∫£n ph·∫©m t·ªët, r·∫•t h√†i l√≤ng\", \"Ch·∫•t l∆∞·ª£ng v∆∞·ª£t mong ƒë·ª£i\", \"ƒê√°ng ti·ªÅn, mua l·∫°i\"]\n",
    "templates_neg = [\"Kh√¥ng nh∆∞ m√¥ t·∫£, th·∫•t v·ªçng\", \"Giao h√†ng h·ªèng\", \"Ch·∫•t l∆∞·ª£ng k√©m\"]\n",
    "\n",
    "outfile = \"itemReview_pxquy.csv\"\n",
    "with open(outfile, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"user_id\",\"item_id\",\"rating\",\"review_text\",\"date\"])\n",
    "    start = datetime(2023,1,1)\n",
    "    for u in users:\n",
    "        for _ in range(REVIEWS_PER_USER):\n",
    "            item = random.choice(items)\n",
    "            p = random.random()\n",
    "            if p < 0.05: rating = 1\n",
    "            elif p < 0.12: rating = 2\n",
    "            elif p < 0.35: rating = 3\n",
    "            elif p < 0.75: rating = 4\n",
    "            else: rating = 5\n",
    "            review = random.choice(templates_pos if rating>=4 else templates_neg if rating<=2 else [\"B√¨nh th∆∞·ªùng, ·ªïn\"])\n",
    "            date = (start + timedelta(days=random.randint(0,1000))).strftime(\"%Y-%m-%d\")\n",
    "            writer.writerow([u, item, rating, review, date])\n",
    "print(\"Saved\", outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686c699e-9b81-4302-9a5f-233042e77709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "vn_stopwords = {\"v√†\",\"l√†\",\"c·ªßa\",\"c√≥\",\"cho\",\"nh·ªØng\",\"ƒë√£\",\"r·∫•t\",\"r·ªìi\",\"v·ªõi\",\"m·ªôt\",\"c√°c\",\"tr√™n\",\"t·ª´\"}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^0-9a-z·∫°√°√†√¢√£·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ√™·∫ø·ªÅ·ªÉ·ªÖƒë√¨√≠ƒ©·ªâ√≤√≥·ªè√µ√¥·ªë·ªì·ªï·ªó∆°·ªõ·ªù·ªü·ª£√π√∫·ªß≈©∆∞·ª©·ª´·ª≠·ªØ·ª≥√Ω·ª∑·ªπ\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokenized = ViTokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokenized.split() if t not in vn_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df = pd.read_csv(\"itemReview_pxquy.csv\")\n",
    "df[\"clean_text\"] = df[\"review_text\"].apply(preprocess_text)\n",
    "df.to_csv(\"itemReview_pxquy_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de1d86c-53a6-4868-9a7b-d89d92a31778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "df = pd.read_csv(\"itemReview_pxquy_preprocessed.csv\")\n",
    "sentences = [s.split() for s in df[\"clean_text\"].astype(str).tolist()]\n",
    "w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=2, epochs=10)\n",
    "w2v.save(\"w2v_pxquy.model\")\n",
    "\n",
    "def doc_vec(s):\n",
    "    toks = s.split()\n",
    "    vecs = [w2v.wv[t] for t in toks if t in w2v.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517ab6f9-90ae-44d5-ab25-b4e523f62f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[\"rating_norm\"] = scaler.fit_transform(df[[\"rating\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dd6206-d84e-45b9-9a52-acf634a2aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating: 3.4881294559238647\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"itemReview_pxquy.csv\")\n",
    "users = {u:i for i,u in enumerate(df['user_id'].unique())}\n",
    "items = {p:i for i,p in enumerate(df['item_id'].unique())}\n",
    "\n",
    "R = np.zeros((len(users), len(items)))\n",
    "for row in df.itertuples():\n",
    "    R[users[row.user_id], items[row.item_id]] = row.rating\n",
    "\n",
    "# Mask missing values (0) ‚Üí ch·ªâ d√πng nh·ªØng √¥ c√≥ rating\n",
    "mask = (R > 0)\n",
    "# Thay th·∫ø gi√° tr·ªã thi·∫øu b·∫±ng trung b√¨nh ng∆∞·ªùi d√πng ƒë·ªÉ gi·∫£m bias\n",
    "mean_user = np.sum(R, axis=1) / np.sum(mask, axis=1)\n",
    "for i in range(R.shape[0]):\n",
    "    R[i, ~mask[i]] = mean_user[i]\n",
    "\n",
    "# Th·ª±c hi·ªán SVD\n",
    "U, sigma, Vt = np.linalg.svd(R, full_matrices=False)\n",
    "k = 50\n",
    "sigma_k = np.diag(sigma[:k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "R_hat = U_k @ sigma_k @ Vt_k  # Ma tr·∫≠n d·ª± ƒëo√°n rating\n",
    "\n",
    "# D·ª± ƒëo√°n rating cho user_0001 v√† item_0001\n",
    "u_idx = users['user_0001']\n",
    "i_idx = items['item_0001']\n",
    "predicted_rating = R_hat[u_idx, i_idx]\n",
    "print(\"Predicted rating:\", predicted_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "383b281a-f5ae-4c54-af0c-0e5d8512b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QUY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad8186a-ce74-4d5f-b414-3e1834e7e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_history(h, name):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(h.history['loss']); plt.plot(h.history['val_loss']); plt.title('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(h.history['accuracy']); plt.plot(h.history['val_accuracy']); plt.title('Accuracy')\n",
    "    plt.suptitle(name)\n",
    "    plt.savefig(name+\"_history.png\")\n",
    "    plt.show()   # üëâ th√™m d√≤ng n√†y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83eff51-8c5c-475e-aaae-b2bd5cd012aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ƒë√£ t·ªìn t·∫°i: itemReview_pxquy.csv\n",
      "K√≠ch th∆∞·ªõc dataset: (50000, 5)\n",
      "ƒê√£ l∆∞u file ti·ªÅn x·ª≠ l√Ω: itemReview_pxquy_preprocessed.csv\n",
      "BERT s·∫µn s√†ng.\n",
      "CF (SVD) RMSE (test): 1.0883\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 6.2 - Full Pipeline for pxquy (a ‚Üí h)\n",
    "# ==========================\n",
    "\n",
    "# --- IMPORTS & CONFIG ---\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# a) Problem/Task Statement\n",
    "# --------------------------\n",
    "# M·ª•c ti√™u: X√¢y d·ª±ng pipeline AI cho h·ªá th·ªëng e-commerce:\n",
    "# - T·∫°o dataset ng∆∞·ªùi d√πng / s·∫£n ph·∫©m / review\n",
    "# - Ti·ªÅn x·ª≠ l√Ω ti·∫øng Vi·ªát, bi·ªÉu di·ªÖn text b·∫±ng BERT ho·∫∑c Word2Vec\n",
    "# - Chu·∫©n h√≥a rating\n",
    "# - D·ª± ƒëo√°n xu h∆∞·ªõng s·∫£n ph·∫©m b·∫±ng Collaborative Filtering (SVD)\n",
    "# - Hu·∫•n luy·ªán CNN / RNN / LSTM ƒë·ªÉ ph√¢n t√≠ch review (sentiment)\n",
    "# - V·∫Ω bi·ªÉu ƒë·ªì loss/accuracy, ch·ªëng overfitting b·∫±ng Dropout & EarlyStopping\n",
    "# - So s√°nh m√¥ h√¨nh ‚Üí ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\n",
    "# - (i) Deploy web s·∫Ω l√†m sau\n",
    "\n",
    "# --------------------------\n",
    "# b) Generate dataset\n",
    "# --------------------------\n",
    "OUTFILE = \"itemReview_pxquy.csv\"\n",
    "if not os.path.exists(OUTFILE):\n",
    "    print(\"T·∫°o dataset synthetic:\", OUTFILE)\n",
    "    NUM_USERS = 1000\n",
    "    NUM_ITEMS = 500\n",
    "    REVIEWS_PER_USER = 50  # 50k d√≤ng\n",
    "    users = [f\"user_{i:04d}\" for i in range(1, NUM_USERS+1)]\n",
    "    items = [f\"item_{i:04d}\" for i in range(1, NUM_ITEMS+1)]\n",
    "    templates_pos = [\n",
    "        \"S·∫£n ph·∫©m t·ªët, r·∫•t h√†i l√≤ng\",\n",
    "        \"Ch·∫•t l∆∞·ª£ng v∆∞·ª£t mong ƒë·ª£i\",\n",
    "        \"Mua l·∫°i l·∫ßn n·ªØa, ƒë√°ng ti·ªÅn\",\n",
    "        \"Giao h√†ng nhanh, ƒë√≥ng g√≥i c·∫©n th·∫≠n\",\n",
    "        \"Thi·∫øt k·∫ø ƒë·∫πp, d√πng ·ªïn\"\n",
    "    ]\n",
    "    templates_neg = [\n",
    "        \"Ch·∫•t l∆∞·ª£ng k√©m, kh√¥ng nh∆∞ m√¥ t·∫£\",\n",
    "        \"Giao h√†ng ch·∫≠m, h·ªèng h√≥c\",\n",
    "        \"Kh√¥ng ƒë√°ng ti·ªÅn, th·∫•t v·ªçng\",\n",
    "        \"S·∫£n ph·∫©m b·ªã l·ªói, c·∫ßn ƒë·ªïi tr·∫£\",\n",
    "        \"D·ªãch v·ª• chƒÉm s√≥c kh√°ch h√†ng t·ªá\"\n",
    "    ]\n",
    "    start = datetime(2023,1,1)\n",
    "    with open(OUTFILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"user_id\",\"item_id\",\"rating\",\"review_text\",\"date\"])\n",
    "        for u in users:\n",
    "            for _ in range(REVIEWS_PER_USER):\n",
    "                item = random.choice(items)\n",
    "                p = random.random()\n",
    "                if p < 0.05:\n",
    "                    rating = 1\n",
    "                elif p < 0.12:\n",
    "                    rating = 2\n",
    "                elif p < 0.35:\n",
    "                    rating = 3\n",
    "                elif p < 0.75:\n",
    "                    rating = 4\n",
    "                else:\n",
    "                    rating = 5\n",
    "                review = random.choice(templates_pos if rating>=4 else templates_neg if rating<=2 else [\"B√¨nh th∆∞·ªùng, kh√¥ng qu√° t·ªá\"])\n",
    "                date = (start + timedelta(days=random.randint(0,1000))).strftime(\"%Y-%m-%d\")\n",
    "                writer.writerow([u, item, rating, review, date])\n",
    "    print(\"Saved:\", OUTFILE)\n",
    "else:\n",
    "    print(\"Dataset ƒë√£ t·ªìn t·∫°i:\", OUTFILE)\n",
    "\n",
    "# --------------------------\n",
    "# c) Preprocessing stopwords + tokenize\n",
    "# --------------------------\n",
    "df = pd.read_csv(OUTFILE)\n",
    "print(\"K√≠ch th∆∞·ªõc dataset:\", df.shape)\n",
    "\n",
    "vn_stopwords = set([\n",
    "    \"v√†\",\"l√†\",\"c·ªßa\",\"c√≥\",\"cho\",\"nh·ªØng\",\"ƒë√£\",\"r·∫•t\",\"r·ªìi\",\"v·ªõi\",\"m·ªôt\",\"c√°c\",\"tr√™n\",\"t·ª´\",\n",
    "    \"kh√¥ng\",\"nh∆∞ng\",\"n√™n\",\"n·∫øu\",\"v√¨\",\"c≈©ng\",\"c·∫£\"\n",
    "])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^0-9a-z·∫°√°√†·∫£√£ƒÉ·∫Ø·∫±·∫≥·∫µ√¢·∫•·∫ß·∫©·∫´ƒë√™·∫ø·ªÅ·ªÉ·ªÖ√¨√≠·ªâƒ©√≤√≥·ªè√µ√¥·ªë·ªì·ªï·ªó∆°·ªõ·ªù·ªü·ª£√π√∫·ªß≈©∆∞·ª©·ª´·ª≠·ªØ·ª≥√Ω·ª∑·ªπ\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = [t for t in text.split() if t not in vn_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_text'] = df['review_text'].apply(preprocess_text)\n",
    "df.to_csv(\"itemReview_pxquy_preprocessed.csv\", index=False)\n",
    "print(\"ƒê√£ l∆∞u file ti·ªÅn x·ª≠ l√Ω:\", \"itemReview_pxquy_preprocessed.csv\")\n",
    "\n",
    "# --------------------------\n",
    "# c) BERT ho·∫∑c Word2Vec\n",
    "# --------------------------\n",
    "USE_BERT = False\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    model_name = \"bert-base-multilingual-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_model = AutoModel.from_pretrained(model_name)\n",
    "    bert_model.eval()\n",
    "    USE_BERT = True\n",
    "    print(\"BERT s·∫µn s√†ng.\")\n",
    "except:\n",
    "    print(\"Kh√¥ng d√πng BERT. S·∫Ω fallback sang Word2Vec.\")\n",
    "\n",
    "if USE_BERT:\n",
    "    sample_texts = df['clean_text'].sample(2000, random_state=SEED).tolist()\n",
    "    def bert_encode(texts, batch_size=16):\n",
    "        embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            with torch.no_grad():\n",
    "                out = bert_model(**inputs)\n",
    "            cls = out.last_hidden_state[:,0,:].cpu().numpy()\n",
    "            embs.append(cls)\n",
    "        return np.vstack(embs)\n",
    "    bert_embs = bert_encode(sample_texts)\n",
    "    np.save(\"bert_pxquy_sample.npy\", bert_embs)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    sentences = [s.split() for s in df['clean_text'].tolist()]\n",
    "    w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
    "    w2v.save(\"w2v_pxquy.model\")\n",
    "    def doc_vector(s):\n",
    "        toks = s.split()\n",
    "        vecs = [w2v.wv[t] for t in toks if t in w2v.wv]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n",
    "    df['w2v_vec'] = df['clean_text'].apply(lambda s: doc_vector(s).tolist())\n",
    "    df.to_pickle(\"itemReview_pxquy_w2v.pkl\")\n",
    "    print(\"ƒê√£ l∆∞u Word2Vec embeddings.\")\n",
    "\n",
    "# --------------------------\n",
    "# d) Normalize ratings\n",
    "# --------------------------\n",
    "scaler = MinMaxScaler()\n",
    "df['rating_norm'] = scaler.fit_transform(df[['rating']])\n",
    "\n",
    "# --------------------------\n",
    "# e) Collaborative Filtering (SVD - NumPy)\n",
    "# --------------------------\n",
    "users = df['user_id'].unique().tolist()\n",
    "items = df['item_id'].unique().tolist()\n",
    "u2i = {u:i for i,u in enumerate(users)}\n",
    "p2i = {p:i for i,p in enumerate(items)}\n",
    "n_u, n_i = len(users), len(items)\n",
    "\n",
    "R = np.zeros((n_u, n_i))\n",
    "for row in df.itertuples():\n",
    "    R[u2i[row.user_id], p2i[row.item_id]] = row.rating\n",
    "\n",
    "# Chia train/test\n",
    "positions = np.array(np.where(R>0)).T\n",
    "np.random.shuffle(positions)\n",
    "test_size = int(len(positions)*0.2)\n",
    "test_pos = positions[:test_size]\n",
    "train_pos = positions[test_size:]\n",
    "\n",
    "R_train = R.copy()\n",
    "for (u,v) in test_pos:\n",
    "    R_train[u,v] = 0\n",
    "\n",
    "# Thay missing b·∫±ng mean user\n",
    "R_filled = R_train.copy()\n",
    "for u in range(n_u):\n",
    "    row = R_train[u]\n",
    "    obs = row[row>0]\n",
    "    mean_u = obs.mean() if len(obs)>0 else df['rating'].mean()\n",
    "    R_filled[u, row==0] = mean_u\n",
    "\n",
    "U, s, Vt = np.linalg.svd(R_filled, full_matrices=False)\n",
    "k = 50\n",
    "R_hat = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(R_true, R_pred, positions):\n",
    "    ys, ys_hat = [], []\n",
    "    for (u,v) in positions:\n",
    "        ys.append(R_true[u,v])\n",
    "        ys_hat.append(R_pred[u,v])\n",
    "    return sqrt(mean_squared_error(ys, ys_hat))\n",
    "\n",
    "\n",
    "rmse_test = rmse(R, R_hat, test_pos)\n",
    "print(f\"CF (SVD) RMSE (test): {rmse_test:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# f) CNN / RNN / LSTM tr√™n review\n",
    "# --------------------------\n",
    "def label_from_rating(r):\n",
    "    return 2 if r>=4 else 1 if r==3 else 0\n",
    "\n",
    "df['label'] = df['rating'].apply(label_from_rating)\n",
    "SAMPLE_SIZE = 15000\n",
    "df_small = df.sample(min(SAMPLE_SIZE, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(df_small['clean_text'])\n",
    "seq = tokenizer.texts_to_sequences(df_small['clean_text'])\n",
    "X = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "y = to_categorical(df_small['label'], num_classes=3)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# H√†m v·∫Ω history\n",
    "def plot_history(h, name):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(h.history['loss']); plt.plot(h.history['val_loss']); plt.title('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(h.history['accuracy']); plt.plot(h.history['val_accuracy']); plt.title('Accuracy')\n",
    "    plt.suptitle(name)\n",
    "    plt.savefig(f\"{name}_pxquy_history.png\")\n",
    "    plt.show()\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, SimpleRNN, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "EMBED_DIM = 128\n",
    "EPOCHS = 6\n",
    "BATCH = 256\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "def build_cnn():\n",
    "    m = Sequential([\n",
    "        Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def build_rnn():\n",
    "    m = Sequential([\n",
    "        Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "        SimpleRNN(64),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def build_lstm():\n",
    "    m = Sequential([\n",
    "        Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c68fda-c963-4674-b501-87e7de1ae70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
