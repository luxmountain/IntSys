{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06b6e3b-6abf-4ed5-99e3-71be8773bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved itemReview_pxquy.csv\n"
     ]
    }
   ],
   "source": [
    "# T·∫° Cao S∆°n - B22DCVT445\n",
    "import csv, random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "NUM_USERS = 1000\n",
    "NUM_ITEMS = 500\n",
    "REVIEWS_PER_USER = 50\n",
    "users = [f\"user_{i:04d}\" for i in range(1, NUM_USERS+1)]\n",
    "items = [f\"item_{i:04d}\" for i in range(1, NUM_ITEMS+1)]\n",
    "\n",
    "templates_pos = [\"S·∫£n ph·∫©m t·ªët, r·∫•t h√†i l√≤ng\", \"Ch·∫•t l∆∞·ª£ng v∆∞·ª£t mong ƒë·ª£i\", \"ƒê√°ng ti·ªÅn, mua l·∫°i\"]\n",
    "templates_neg = [\"Kh√¥ng nh∆∞ m√¥ t·∫£, th·∫•t v·ªçng\", \"Giao h√†ng h·ªèng\", \"Ch·∫•t l∆∞·ª£ng k√©m\"]\n",
    "\n",
    "outfile = \"itemReview_pxquy.csv\"\n",
    "with open(outfile, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"user_id\",\"item_id\",\"rating\",\"review_text\",\"date\"])\n",
    "    start = datetime(2023,1,1)\n",
    "    for u in users:\n",
    "        for _ in range(REVIEWS_PER_USER):\n",
    "            item = random.choice(items)\n",
    "            p = random.random()\n",
    "            if p < 0.05: rating = 1\n",
    "            elif p < 0.12: rating = 2\n",
    "            elif p < 0.35: rating = 3\n",
    "            elif p < 0.75: rating = 4\n",
    "            else: rating = 5\n",
    "            review = random.choice(templates_pos if rating>=4 else templates_neg if rating<=2 else [\"B√¨nh th∆∞·ªùng, ·ªïn\"])\n",
    "            date = (start + timedelta(days=random.randint(0,1000))).strftime(\"%Y-%m-%d\")\n",
    "            writer.writerow([u, item, rating, review, date])\n",
    "print(\"Saved\", outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686c699e-9b81-4302-9a5f-233042e77709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "vn_stopwords = {\"v√†\",\"l√†\",\"c·ªßa\",\"c√≥\",\"cho\",\"nh·ªØng\",\"ƒë√£\",\"r·∫•t\",\"r·ªìi\",\"v·ªõi\",\"m·ªôt\",\"c√°c\",\"tr√™n\",\"t·ª´\"}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^0-9a-z·∫°√°√†√¢√£·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ√™·∫ø·ªÅ·ªÉ·ªÖƒë√¨√≠ƒ©·ªâ√≤√≥·ªè√µ√¥·ªë·ªì·ªï·ªó∆°·ªõ·ªù·ªü·ª£√π√∫·ªß≈©∆∞·ª©·ª´·ª≠·ªØ·ª≥√Ω·ª∑·ªπ\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokenized = ViTokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokenized.split() if t not in vn_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df = pd.read_csv(\"itemReview_pxquy.csv\")\n",
    "df[\"clean_text\"] = df[\"review_text\"].apply(preprocess_text)\n",
    "df.to_csv(\"itemReview_pxquy_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de1d86c-53a6-4868-9a7b-d89d92a31778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 22 word vectors\n",
      "‚úÖ Simple Word2Vec training completed (no C++ compiler needed!)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Simple Word2Vec class (kh√¥ng c·∫ßn gensim)\n",
    "class SimpleWord2Vec:\n",
    "    def __init__(self, vector_size=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.word_vectors = {}\n",
    "        \n",
    "    def train_from_sentences(self, sentences):\n",
    "        # T·∫°o corpus t·ª´ sentences\n",
    "        corpus = [' '.join(sent) for sent in sentences if len(sent) > 0]\n",
    "        \n",
    "        # TF-IDF vectorization\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1))\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # SVD ƒë·ªÉ gi·∫£m chi·ªÅu\n",
    "        svd = TruncatedSVD(n_components=self.vector_size, random_state=42)\n",
    "        word_vectors_matrix = svd.fit_transform(tfidf_matrix.T)\n",
    "        \n",
    "        # Mapping t·ª´ -> vector\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for i, word in enumerate(feature_names):\n",
    "            if i < len(word_vectors_matrix):\n",
    "                self.word_vectors[word] = word_vectors_matrix[i]\n",
    "        \n",
    "        print(f\"Trained {len(self.word_vectors)} word vectors\")\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        return self.word_vectors.get(word, np.zeros(self.vector_size))\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        np.savez(filepath, word_vectors=self.word_vectors, vector_size=self.vector_size)\n",
    "\n",
    "# Load data v√† train\n",
    "df = pd.read_csv(\"itemReview_pxquy_preprocessed.csv\")\n",
    "sentences = [s.split() for s in df[\"clean_text\"].astype(str).tolist() if len(s.strip()) > 0]\n",
    "\n",
    "# Train simple word2vec\n",
    "w2v = SimpleWord2Vec(vector_size=100)\n",
    "w2v.train_from_sentences(sentences)\n",
    "w2v.save(\"simple_w2v_pxquy.npz\")\n",
    "\n",
    "def doc_vec(s):\n",
    "    \"\"\"T·∫°o document vector b·∫±ng c√°ch average word vectors\"\"\"\n",
    "    toks = s.split()\n",
    "    vecs = [w2v.get_vector(t) for t in toks if t in w2v.word_vectors]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n",
    "\n",
    "print(\"‚úÖ Simple Word2Vec training completed (no C++ compiler needed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517ab6f9-90ae-44d5-ab25-b4e523f62f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[\"rating_norm\"] = scaler.fit_transform(df[[\"rating\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38dd6206-d84e-45b9-9a52-acf634a2aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating: 3.2766736378431296\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"itemReview_pxquy.csv\")\n",
    "users = {u:i for i,u in enumerate(df['user_id'].unique())}\n",
    "items = {p:i for i,p in enumerate(df['item_id'].unique())}\n",
    "\n",
    "R = np.zeros((len(users), len(items)))\n",
    "for row in df.itertuples():\n",
    "    R[users[row.user_id], items[row.item_id]] = row.rating\n",
    "\n",
    "# Mask missing values (0) ‚Üí ch·ªâ d√πng nh·ªØng √¥ c√≥ rating\n",
    "mask = (R > 0)\n",
    "# Thay th·∫ø gi√° tr·ªã thi·∫øu b·∫±ng trung b√¨nh ng∆∞·ªùi d√πng ƒë·ªÉ gi·∫£m bias\n",
    "mean_user = np.sum(R, axis=1) / np.sum(mask, axis=1)\n",
    "for i in range(R.shape[0]):\n",
    "    R[i, ~mask[i]] = mean_user[i]\n",
    "\n",
    "# Th·ª±c hi·ªán SVD\n",
    "U, sigma, Vt = np.linalg.svd(R, full_matrices=False)\n",
    "k = 50\n",
    "sigma_k = np.diag(sigma[:k])\n",
    "U_k = U[:, :k]\n",
    "Vt_k = Vt[:k, :]\n",
    "R_hat = U_k @ sigma_k @ Vt_k  # Ma tr·∫≠n d·ª± ƒëo√°n rating\n",
    "\n",
    "# D·ª± ƒëo√°n rating cho user_0001 v√† item_0001\n",
    "u_idx = users['user_0001']\n",
    "i_idx = items['item_0001']\n",
    "predicted_rating = R_hat[u_idx, i_idx]\n",
    "print(\"Predicted rating:\", predicted_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383b281a-f5ae-4c54-af0c-0e5d8512b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad8186a-ce74-4d5f-b414-3e1834e7e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_history(h, name):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(h.history['loss']); plt.plot(h.history['val_loss']); plt.title('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(h.history['accuracy']); plt.plot(h.history['val_accuracy']); plt.title('Accuracy')\n",
    "    plt.suptitle(name)\n",
    "    plt.savefig(name+\"_history.png\")\n",
    "    plt.show()   # üëâ th√™m d√≤ng n√†y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83eff51-8c5c-475e-aaae-b2bd5cd012aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ M·ª•c ti√™u: X√¢y d·ª±ng pipeline AI cho e-commerce (KH√îNG D√ôNG GENSIM)\n",
      "- T·∫°o dataset + ti·ªÅn x·ª≠ l√Ω ti·∫øng Vi·ªát\n",
      "- Word embedding ƒë∆°n gi·∫£n (TF-IDF + SVD)\n",
      "- Collaborative Filtering (SVD)\n",
      "- CNN/RNN/LSTM sentiment analysis\n",
      "- So s√°nh v√† ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\n",
      "üìà Dataset size: (50000, 5)\n",
      "‚úÖ Text preprocessing completed\n",
      "üî§ Trained embeddings for 26 words\n",
      "ü§ù Training Collaborative Filtering...\n",
      "üìä CF RMSE: 1.0829\n",
      "üß† Training Deep Learning Models...\n",
      "üèãÔ∏è Training CNN...\n",
      "üìà CNN - Val Accuracy: 1.0000, Val Loss: 0.0000\n",
      "üèãÔ∏è Training LSTM...\n",
      "üìà LSTM - Val Accuracy: 1.0000, Val Loss: 0.0000\n",
      "üèãÔ∏è Training GRU...\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 6.2 - Full Pipeline for pxquy (a ‚Üí h) - NO GENSIM VERSION\n",
    "# ==========================\n",
    "\n",
    "# --- IMPORTS & CONFIG ---\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# a) Problem/Task Statement\n",
    "# --------------------------\n",
    "print(\"üéØ M·ª•c ti√™u: X√¢y d·ª±ng pipeline AI cho e-commerce (KH√îNG D√ôNG GENSIM)\")\n",
    "print(\"- T·∫°o dataset + ti·ªÅn x·ª≠ l√Ω ti·∫øng Vi·ªát\")\n",
    "print(\"- Word embedding ƒë∆°n gi·∫£n (TF-IDF + SVD)\")\n",
    "print(\"- Collaborative Filtering (SVD)\")\n",
    "print(\"- CNN/RNN/LSTM sentiment analysis\")\n",
    "print(\"- So s√°nh v√† ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\")\n",
    "\n",
    "# --------------------------\n",
    "# b) Generate dataset\n",
    "# --------------------------\n",
    "OUTFILE = \"itemReview_pxquy.csv\"\n",
    "if not os.path.exists(OUTFILE):\n",
    "    print(\"üìä T·∫°o dataset synthetic...\")\n",
    "    NUM_USERS = 1000\n",
    "    NUM_ITEMS = 500\n",
    "    REVIEWS_PER_USER = 50\n",
    "    users = [f\"user_{i:04d}\" for i in range(1, NUM_USERS+1)]\n",
    "    items = [f\"item_{i:04d}\" for i in range(1, NUM_ITEMS+1)]\n",
    "    \n",
    "    templates_pos = [\n",
    "        \"S·∫£n ph·∫©m t·ªët, r·∫•t h√†i l√≤ng v·ªõi ch·∫•t l∆∞·ª£ng\",\n",
    "        \"Ch·∫•t l∆∞·ª£ng v∆∞·ª£t mong ƒë·ª£i, giao h√†ng nhanh\",\n",
    "        \"Mua l·∫°i l·∫ßn n·ªØa, ƒë√°ng ti·ªÅn, thi·∫øt k·∫ø ƒë·∫πp\",\n",
    "        \"D·ªãch v·ª• tuy·ªát v·ªùi, ƒë√≥ng g√≥i c·∫©n th·∫≠n\",\n",
    "        \"R·∫•t h√†i l√≤ng, ch·∫•t l∆∞·ª£ng cao, gi√° h·ª£p l√Ω\"\n",
    "    ]\n",
    "    templates_neg = [\n",
    "        \"Ch·∫•t l∆∞·ª£ng k√©m, kh√¥ng nh∆∞ m√¥ t·∫£, th·∫•t v·ªçng\",\n",
    "        \"Giao h√†ng ch·∫≠m, s·∫£n ph·∫©m b·ªã h·ªèng\",\n",
    "        \"Kh√¥ng ƒë√°ng ti·ªÅn, d·ªãch v·ª• t·ªá\",\n",
    "        \"S·∫£n ph·∫©m l·ªói, c·∫ßn ƒë·ªïi tr·∫£ ngay\",\n",
    "        \"R·∫•t kh√¥ng h√†i l√≤ng, ch·∫•t l∆∞·ª£ng d∆∞·ªõi mong ƒë·ª£i\"\n",
    "    ]\n",
    "    \n",
    "    with open(OUTFILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"user_id\",\"item_id\",\"rating\",\"review_text\",\"date\"])\n",
    "        start = datetime(2023,1,1)\n",
    "        for u in users:\n",
    "            for _ in range(REVIEWS_PER_USER):\n",
    "                item = random.choice(items)\n",
    "                p = random.random()\n",
    "                if p < 0.05: rating = 1\n",
    "                elif p < 0.12: rating = 2\n",
    "                elif p < 0.35: rating = 3\n",
    "                elif p < 0.75: rating = 4\n",
    "                else: rating = 5\n",
    "                \n",
    "                review = random.choice(templates_pos if rating>=4 else templates_neg if rating<=2 else [\"B√¨nh th∆∞·ªùng, ·ªïn\"])\n",
    "                date = (start + timedelta(days=random.randint(0,1000))).strftime(\"%Y-%m-%d\")\n",
    "                writer.writerow([u, item, rating, review, date])\n",
    "    print(f\"‚úÖ Saved: {OUTFILE}\")\n",
    "\n",
    "# --------------------------\n",
    "# c) Preprocessing (kh√¥ng c·∫ßn pyvi)\n",
    "# --------------------------\n",
    "df = pd.read_csv(OUTFILE)\n",
    "print(f\"üìà Dataset size: {df.shape}\")\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"Ti·ªÅn x·ª≠ l√Ω ƒë∆°n gi·∫£n kh√¥ng c·∫ßn th∆∞ vi·ªán ngo√†i\"\"\"\n",
    "    text = str(text).lower()\n",
    "    # Ch·ªâ gi·ªØ ch·ªØ c√°i Vi·ªát v√† s·ªë\n",
    "    text = re.sub(r'[^a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = {'v√†','l√†','c·ªßa','c√≥','cho','nh·ªØng','ƒë√£','r·∫•t','r·ªìi','v·ªõi','m·ªôt','c√°c','tr√™n','t·ª´','kh√¥ng','r·∫±ng','n√†y','ƒë√≥'}\n",
    "    tokens = [w for w in text.split() if w not in stopwords and len(w) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['review_text'].apply(simple_preprocess)\n",
    "print(\"‚úÖ Text preprocessing completed\")\n",
    "\n",
    "# --------------------------\n",
    "# d) Simple Word2Vec (TF-IDF + SVD)\n",
    "# --------------------------\n",
    "class SimpleWord2Vec:\n",
    "    def __init__(self, vector_size=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.word_vectors = {}\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        # T·∫°o corpus\n",
    "        valid_sentences = [sent for sent in sentences if len(sent) > 2]\n",
    "        corpus = [' '.join(sent) for sent in valid_sentences]\n",
    "        \n",
    "        if not corpus:\n",
    "            print(\"‚ùå No valid sentences for training!\")\n",
    "            return\n",
    "            \n",
    "        # TF-IDF\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, min_df=2, ngram_range=(1,1))\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # SVD\n",
    "        svd = TruncatedSVD(n_components=self.vector_size, random_state=SEED)\n",
    "        word_embeddings = svd.fit_transform(tfidf_matrix.T)\n",
    "        \n",
    "        # Map words to vectors\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for i, word in enumerate(feature_names):\n",
    "            if i < len(word_embeddings):\n",
    "                self.word_vectors[word] = word_embeddings[i]\n",
    "        \n",
    "        print(f\"üî§ Trained embeddings for {len(self.word_vectors)} words\")\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        return self.word_vectors.get(word, np.zeros(self.vector_size))\n",
    "\n",
    "# Train word embeddings\n",
    "sentences = [text.split() for text in df['clean_text'] if len(text.strip()) > 0]\n",
    "w2v = SimpleWord2Vec(vector_size=100)\n",
    "w2v.train(sentences)\n",
    "\n",
    "def doc_vector(text):\n",
    "    tokens = text.split()\n",
    "    vectors = [w2v.get_vector(t) for t in tokens if t in w2v.word_vectors]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "df['doc_embedding'] = df['clean_text'].apply(doc_vector)\n",
    "\n",
    "# --------------------------\n",
    "# e) Rating normalization\n",
    "# --------------------------\n",
    "scaler = MinMaxScaler()\n",
    "df['rating_norm'] = scaler.fit_transform(df[['rating']])\n",
    "\n",
    "# --------------------------\n",
    "# f) Collaborative Filtering (SVD)\n",
    "# --------------------------\n",
    "print(\"ü§ù Training Collaborative Filtering...\")\n",
    "users = df['user_id'].unique()\n",
    "items = df['item_id'].unique()\n",
    "u2i = {u:i for i,u in enumerate(users)}\n",
    "i2i = {item:i for i,item in enumerate(items)}\n",
    "\n",
    "R = np.zeros((len(users), len(items)))\n",
    "for row in df.itertuples():\n",
    "    R[u2i[row.user_id], i2i[row.item_id]] = row.rating\n",
    "\n",
    "# Train/test split\n",
    "positions = np.array(np.where(R > 0)).T\n",
    "np.random.shuffle(positions)\n",
    "test_size = int(len(positions) * 0.2)\n",
    "test_pos = positions[:test_size]\n",
    "train_pos = positions[test_size:]\n",
    "\n",
    "R_train = R.copy()\n",
    "for (u, i) in test_pos:\n",
    "    R_train[u, i] = 0\n",
    "\n",
    "# Fill missing with user means\n",
    "R_filled = R_train.copy()\n",
    "for u in range(len(users)):\n",
    "    user_ratings = R_train[u][R_train[u] > 0]\n",
    "    user_mean = user_ratings.mean() if len(user_ratings) > 0 else df['rating'].mean()\n",
    "    R_filled[u][R_train[u] == 0] = user_mean\n",
    "\n",
    "# SVD decomposition\n",
    "U, s, Vt = np.linalg.svd(R_filled, full_matrices=False)\n",
    "k = 50\n",
    "R_pred = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "# Evaluate\n",
    "def rmse(true_ratings, pred_ratings, positions):\n",
    "    true_vals = [true_ratings[u, i] for (u, i) in positions]\n",
    "    pred_vals = [pred_ratings[u, i] for (u, i) in positions]\n",
    "    return np.sqrt(mean_squared_error(true_vals, pred_vals))\n",
    "\n",
    "cf_rmse = rmse(R, R_pred, test_pos)\n",
    "print(f\"üìä CF RMSE: {cf_rmse:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# g) Deep Learning Models (CNN/RNN/LSTM)\n",
    "# --------------------------\n",
    "print(\"üß† Training Deep Learning Models...\")\n",
    "\n",
    "# Prepare data\n",
    "def rating_to_sentiment(rating):\n",
    "    return 2 if rating >= 4 else 1 if rating == 3 else 0\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(rating_to_sentiment)\n",
    "\n",
    "# Sample for faster training\n",
    "SAMPLE_SIZE = 10000\n",
    "df_sample = df.sample(min(SAMPLE_SIZE, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 80\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(df_sample['clean_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df_sample['clean_text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "y = to_categorical(df_sample['sentiment'], 3)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Model architectures\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_cnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "        LSTM(64, dropout=0.5, recurrent_dropout=0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model():\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "        GRU(64, dropout=0.5, recurrent_dropout=0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_model(model, name):\n",
    "    print(f\"üèãÔ∏è Training {name}...\")\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"üìà {name} - Val Accuracy: {val_acc:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history, val_acc\n",
    "\n",
    "# Train all models\n",
    "models_results = {}\n",
    "\n",
    "# CNN\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_history, cnn_acc = train_model(cnn_model, \"CNN\")\n",
    "models_results['CNN'] = cnn_acc\n",
    "\n",
    "# LSTM\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_history, lstm_acc = train_model(lstm_model, \"LSTM\")\n",
    "models_results['LSTM'] = lstm_acc\n",
    "\n",
    "# GRU\n",
    "gru_model = build_gru_model()\n",
    "gru_history, gru_acc = train_model(gru_model, \"GRU\")\n",
    "models_results['GRU'] = gru_acc\n",
    "\n",
    "# --------------------------\n",
    "# h) Results Summary\n",
    "# --------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä Dataset: {len(df)} reviews, {len(users)} users, {len(items)} items\")\n",
    "print(f\"ü§ù Collaborative Filtering RMSE: {cf_rmse:.4f}\")\n",
    "print(\"\\nüìà Deep Learning Models:\")\n",
    "\n",
    "best_model = max(models_results, key=models_results.get)\n",
    "for model_name, accuracy in sorted(models_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    status = \"ü•á BEST\" if model_name == best_model else \"\"\n",
    "    print(f\"   {model_name}: {accuracy:.4f} {status}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best performing model: {best_model} ({models_results[best_model]:.4f})\")\n",
    "print(\"üéâ Pipeline completed successfully WITHOUT gensim!\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(models_results.keys())\n",
    "accuracies = list(models_results.values())\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Model Comparison - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_no_gensim.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìÅ Saved: model_comparison_no_gensim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a42824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Loaded 50000 reviews\n",
      "Training Simple Word2Vec...\n",
      "Training on 38517 sentences\n",
      "Vocab size: 25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components(100) must be <= n_features(20).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    148\u001b[39m w2v_simple = SimpleWord2Vec(vector_size=\u001b[32m100\u001b[39m, window=\u001b[32m5\u001b[39m, min_count=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[43mw2v_simple\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m    152\u001b[39m w2v_simple.save(\u001b[33m\"\u001b[39m\u001b[33msimple_w2v_pxquy.npz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mSimpleWord2Vec.train\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[32m     62\u001b[39m svd = TruncatedSVD(n_components=\u001b[38;5;28mself\u001b[39m.vector_size, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m reduced_vectors = \u001b[43msvd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Mapping words to vectors\u001b[39;00m\n\u001b[32m     66\u001b[39m feature_names = vectorizer.get_feature_names_out()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:240\u001b[39m, in \u001b[36mTruncatedSVD.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.algorithm == \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_components > X.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    241\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be <=\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m         )\n\u001b[32m    244\u001b[39m     U, Sigma, VT = _randomized_svd(\n\u001b[32m    245\u001b[39m         X,\n\u001b[32m    246\u001b[39m         \u001b[38;5;28mself\u001b[39m.n_components,\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m         flip_sign=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    252\u001b[39m     )\n\u001b[32m    253\u001b[39m     U, VT = svd_flip(U, VT, u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: n_components(100) must be <= n_features(20)."
     ]
    }
   ],
   "source": [
    "# T·∫° Cao S∆°n - B22DCVT445\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# ================================\n",
    "# 1. Simple Word2Vec alternative using TF-IDF + SVD\n",
    "# ================================\n",
    "\n",
    "class SimpleWord2Vec:\n",
    "    def __init__(self, vector_size=100, window=5, min_count=2):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.word_vectors = {}\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"X√¢y d·ª±ng t·ª´ v·ª±ng t·ª´ danh s√°ch c√¢u\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                word_counts[word] += 1\n",
    "        \n",
    "        # L·ªçc t·ª´ theo min_count\n",
    "        self.vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items()) \n",
    "                      if count >= self.min_count}\n",
    "        print(f\"Vocab size: {len(self.vocab)}\")\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Hu·∫•n luy·ªán Word2Vec ƒë∆°n gi·∫£n b·∫±ng TF-IDF + SVD\"\"\"\n",
    "        self.build_vocab(sentences)\n",
    "        \n",
    "        # T·∫°o context windows\n",
    "        contexts = []\n",
    "        for sentence in sentences:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                if target_word in self.vocab:\n",
    "                    # L·∫•y context words trong window\n",
    "                    start = max(0, i - self.window)\n",
    "                    end = min(len(sentence), i + self.window + 1)\n",
    "                    context = []\n",
    "                    for j in range(start, end):\n",
    "                        if i != j and sentence[j] in self.vocab:\n",
    "                            context.append(sentence[j])\n",
    "                    if context:\n",
    "                        contexts.append(' '.join(context))\n",
    "        \n",
    "        if not contexts:\n",
    "            print(\"Kh√¥ng c√≥ context n√†o ƒë∆∞·ª£c t·∫°o!\")\n",
    "            return\n",
    "        \n",
    "        # S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ t·∫°o word vectors\n",
    "        vectorizer = TfidfVectorizer(max_features=min(5000, len(self.vocab)))\n",
    "        tfidf_matrix = vectorizer.fit_transform(contexts)\n",
    "        \n",
    "        # SVD ƒë·ªÉ gi·∫£m chi·ªÅu\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        svd = TruncatedSVD(n_components=self.vector_size, random_state=42)\n",
    "        reduced_vectors = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        # Mapping words to vectors\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for i, word in enumerate(feature_names):\n",
    "            if i < len(reduced_vectors):\n",
    "                self.word_vectors[word] = reduced_vectors[i]\n",
    "        \n",
    "        print(f\"Trained vectors for {len(self.word_vectors)} words\")\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        \"\"\"L·∫•y vector c·ªßa m·ªôt t·ª´\"\"\"\n",
    "        return self.word_vectors.get(word, np.zeros(self.vector_size))\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"L∆∞u model\"\"\"\n",
    "        np.savez(filepath, \n",
    "                 word_vectors=self.word_vectors,\n",
    "                 vocab=self.vocab,\n",
    "                 vector_size=self.vector_size)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "# ================================\n",
    "# 2. Document vectorization function\n",
    "# ================================\n",
    "\n",
    "def doc_vector_simple(text, word2vec_model, method='mean'):\n",
    "    \"\"\"T·∫°o document vector t·ª´ text b·∫±ng c√°ch average word vectors\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        tokens = text\n",
    "    \n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        vec = word2vec_model.get_vector(token)\n",
    "        if np.any(vec):  # N·∫øu vector kh√¥ng ph·∫£i to√†n s·ªë 0\n",
    "            vectors.append(vec)\n",
    "    \n",
    "    if vectors:\n",
    "        if method == 'mean':\n",
    "            return np.mean(vectors, axis=0)\n",
    "        elif method == 'sum':\n",
    "            return np.sum(vectors, axis=0)\n",
    "    \n",
    "    return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# ================================\n",
    "# 3. Load v√† preprocess data\n",
    "# ================================\n",
    "\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"itemReview_pxquy_preprocessed.csv\")\n",
    "    print(f\"Loaded {len(df)} reviews\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File preprocessed kh√¥ng t·ªìn t·∫°i, s·ª≠ d·ª•ng file g·ªëc...\")\n",
    "    df = pd.read_csv(\"itemReview_pxquy.csv\")\n",
    "    \n",
    "    # Simple preprocessing kh√¥ng c·∫ßn pyvi\n",
    "    def simple_preprocess(text):\n",
    "        text = str(text).lower()\n",
    "        # Ch·ªâ gi·ªØ ch·ªØ c√°i ti·∫øng Vi·ªát v√† s·ªë\n",
    "        text = re.sub(r'[^a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Remove stopwords ƒë∆°n gi·∫£n\n",
    "        stopwords = {'v√†', 'l√†', 'c·ªßa', 'c√≥', 'cho', 'nh·ªØng', 'ƒë√£', 'r·∫•t', 'r·ªìi', 'v·ªõi', 'm·ªôt', 'c√°c', 'tr√™n', 't·ª´'}\n",
    "        tokens = [word for word in text.split() if word not in stopwords and len(word) > 1]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    df['clean_text'] = df['review_text'].apply(simple_preprocess)\n",
    "\n",
    "# ================================\n",
    "# 4. Train Simple Word2Vec\n",
    "# ================================\n",
    "\n",
    "print(\"Training Simple Word2Vec...\")\n",
    "sentences = [text.split() for text in df['clean_text'].astype(str) if len(text.strip()) > 0]\n",
    "\n",
    "# Lo·∫°i b·ªè c√¢u qu√° ng·∫Øn\n",
    "sentences = [s for s in sentences if len(s) > 2]\n",
    "print(f\"Training on {len(sentences)} sentences\")\n",
    "\n",
    "# Train model\n",
    "w2v_simple = SimpleWord2Vec(vector_size=100, window=5, min_count=2)\n",
    "w2v_simple.train(sentences)\n",
    "\n",
    "# Save model\n",
    "w2v_simple.save(\"simple_w2v_pxquy.npz\")\n",
    "\n",
    "# ================================\n",
    "# 5. Create document vectors\n",
    "# ================================\n",
    "\n",
    "print(\"Creating document vectors...\")\n",
    "df['w2v_vector'] = df['clean_text'].apply(lambda x: doc_vector_simple(x, w2v_simple))\n",
    "\n",
    "# Convert to list for saving\n",
    "df['w2v_vector_list'] = df['w2v_vector'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Save with vectors\n",
    "df.to_pickle(\"itemReview_pxquy_simple_w2v.pkl\")\n",
    "print(\"Saved dataframe with simple word2vec vectors\")\n",
    "\n",
    "# ================================\n",
    "# 6. Test similarity function\n",
    "# ================================\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"T√≠nh cosine similarity gi·ªØa 2 vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Test v·ªõi m·ªôt s·ªë t·ª´\n",
    "test_words = ['t·ªët', 'x·∫•u', 'ch·∫•t', 'l∆∞·ª£ng', 'h√†i', 'l√≤ng']\n",
    "print(\"\\n=== Test Word Vectors ===\")\n",
    "for word in test_words:\n",
    "    vec = w2v_simple.get_vector(word)\n",
    "    print(f\"'{word}': vector shape {vec.shape}, norm: {np.linalg.norm(vec):.3f}\")\n",
    "\n",
    "# Test document similarity\n",
    "print(\"\\n=== Test Document Similarity ===\")\n",
    "if len(df) >= 2:\n",
    "    vec1 = df['w2v_vector'].iloc[0]\n",
    "    vec2 = df['w2v_vector'].iloc[1]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(f\"Similarity between doc 0 and doc 1: {similarity:.3f}\")\n",
    "    print(f\"Doc 0: {df['clean_text'].iloc[0][:100]}...\")\n",
    "    print(f\"Doc 1: {df['clean_text'].iloc[1][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Simple Word2Vec training completed successfully!\")\n",
    "print(\"üìÅ Files created:\")\n",
    "print(\"   - simple_w2v_pxquy.npz (Word2Vec model)\")\n",
    "print(\"   - itemReview_pxquy_simple_w2v.pkl (DataFrame with vectors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9dc33-dae7-4e52-a047-a706ae82b372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
